# # -*- coding: utf-8 -*-
# """gg_universal_embed.ipynb
#
# Automatically generated by Colaboratory.
#
# Original file is located at
#     https://colab.research.google.com/drive/1xOk0DYmTooyU_VAAxyr4-4c2UrvkTZf_
# """
#
# # # Install the latest Tensorflow version.
# # !pip3 install --quiet "tensorflow>=1.7"
# # # Install TF-Hub.
# # !pip3 install --quiet tensorflow-hub
# # !pip3 install --quiet seaborn
#
# # from google.colab import drive
# # drive.mount('/content/gdrive',  force_remount=True)
# #
# # !unzip -qq "/content/gdrive/My Drive/20182_DOAN/cnn_eng/data_labels.zip"
# #
# # !unzip -qq "/content/gdrive/My Drive/20182_DOAN/universal_embed/test.zip"
#
# import tensorflow as tf
# import tensorflow_hub as hub
# module_url = '/content/gdrive/My Drive/20182_DOAN/universal_embed/model_tf_hub' #"https://tfhub.dev/google/universal-sentence-encoder-large/3"
# # Import the Universal Sentence Encoder's TF Hub module
# embed = hub.Module(module_url)
#
# import os
# import re
# import pandas as pd
# import numpy as np
# from keras.models import Sequential
# from keras import layers
# from keras.models import Model
# from keras import backend as K
# from keras.callbacks import ModelCheckpoint
# from sklearn.utils import shuffle
#
# checkpointer = ModelCheckpoint(filepath='/content/gdrive/My Drive/20182_DOAN/universal_embed/weights_2.hdf5', verbose=1, save_best_only=True)
#
# def get_dataframe(filedir):
#
#     print(len(os.listdir(filedir)))
#     data = []
#     listfilenames = os.listdir(filedir)[:1]
#     for file in listfilenames:
#         docs = open(filedir + '/' + file, 'r').read().strip().split('\n####\n')
#         for doc in docs:
#             lines = doc.strip().split('\n')
#             lines = [s for s in lines if s != '']
#             for i in range(0, len(lines)):
#                 label = int(lines[i][0])
#                 text = lines[i][2:]
#                 text = re.sub('[^A-Za-z0-9 ,\?\'\"-._\+\!/\`@=;:]+', '', text)
#                 data.append([label, text])
# #     sample = []
# #     for l,t in data:
# #         if l == 1:
# #             sample.append([l,t])
#
# #     data += 10 * sample
#     #data = shuffle(data, random_state=42)
#
#     df = pd.DataFrame(data, columns=['label', 'text'])
#     df.label = df.label.astype('category')
#     return df
#
# df_train = get_dataframe('/content/data_labels/train')
# df_train.head()
#
# train_text = df_train['text'].tolist()
# train_text = np.array(train_text, dtype=object)[:, np.newaxis]
# category_counts = 2
# train_label = np.asarray(pd.get_dummies(df_train.label), dtype = np.int8)
#
# def UniversalEmbedding(x):
#     return embed(tf.squeeze(tf.cast(x, tf.string)),
#     	signature="default", as_dict=True)["default"]
#
# input_text = layers.Input(shape=(1,), dtype="string")
# embedding = layers.Lambda(UniversalEmbedding,
# 	output_shape=(512,))(input_text)
# dense = layers.Dense(256, activation='relu')(embedding)
# pred = layers.Dense(category_counts, activation='softmax')(dense)
# model = Model(inputs=[input_text], outputs=pred)
# model.compile(loss='categorical_crossentropy',
# 	optimizer='adam', metrics=['accuracy'])
#
# def get_dataframe_valid(filedir):
#     list_filenames = os.listdir(filedir)
#     data = []
#     for file in list_filenames:
#         lines = open(filedir + '/' + file, 'r').read().strip().splitlines()
#         for line in lines:
#             label = int(line[0])
#             text = line[2:]
#             text = re.sub('[^A-Za-z0-9 ,\?\'\"-._\+\!/\`@=;:]+', '', text)
#             data.append([label, text])
#
#     df = pd.DataFrame(data, columns=['label', 'text'])
#     df.label = df.label.astype('category')
#     return df
#
# df_test = get_dataframe_valid('/content/data_labels/valid')
# test_text = df_test['text'].tolist()
# test_text = np.array(test_text, dtype=object)[:, np.newaxis]
# test_label = np.asarray(pd.get_dummies(df_test.label), dtype = np.int8)
#
# with tf.Session() as session:
#   K.set_session(session)
#   session.run(tf.global_variables_initializer())
#   session.run(tf.tables_initializer())
#   history = model.fit(train_text,
#             train_label,
#             validation_data=(test_text, test_label),
#             epochs=10,
#             batch_size=60,
#             callbacks=[checkpointer])
#   model.save_weights('/content/gdrive/My Drive/20182_DOAN/universal_embed/model.h5')
#
# filedir = '/content/test'
# def get_dataframe_test(filename):
#     data = []
#     lines = open(filedir + '/' + file, 'r').read().strip().split('\n')
#     lines = [line for line in lines if line != '']
#     for line in lines:
#         label = int(line[0])
#         text = line[2:]
#         text = re.sub('[^A-Za-z0-9 ,\?\'\"-._\+\!/\`@=;:]+', '', text)
#         text = re.sub(r'\s+', ' ', text)
# #         text = text.replace(" \'s", "\'s")
# #         text = text.replace(" \'d", "\'d")
# #         text = text.replace(" \'m", "\'m")
# #         text = text.replace(" n\'t", "n\'t")
#         if text != ' ':
#             data.append([label, text])
#
#     df = pd.DataFrame(data, columns=['label', 'text'])
#     df.label = df.label.astype('category')
#     return df
#
#
# list_test_filenames = os.listdir(filedir)
# with tf.Session() as session:
#     K.set_session(session)
#     session.run(tf.global_variables_initializer())
#     session.run(tf.tables_initializer())
#     model1 = Model(inputs=[input_text], outputs=pred)
#     model1.load_weights('/content/gdrive/My Drive/20182_DOAN/universal_embed/weights.hdf5')
#     model1.compile(loss='categorical_crossentropy',
#     optimizer='adam', metrics=['accuracy'])
#
#     for file in list_test_filenames:
#         try:
#             print(file)
#             df_test = get_dataframe_test(file)
#             new_text = df_test['text'].tolist()
#             new_text = np.array(new_text, dtype=object)[:, np.newaxis]
#             #test_label = np.asarray(pd.get_dummies(df_test.label), dtype = np.int8)
#
#             predicts = model1.predict(new_text, batch_size=80)
#             print(len(new_text), len(predicts))
#             np.save('/content/gdrive/My Drive/20182_DOAN/universal_embed/test_prob_two/' + file + '.npy', predicts)
#         except Exception as e:
#             print(e)
#
# #     categories = df_train.label.cat.categories.tolist()
# #     predict_logits = predicts.argmax(axis=1)
# #     predict_labels = [categories[logit] for logit in predict_logits]
